\documentclass[11pt, oneside, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[sectionbib, round]{natbib}
\usepackage[textwidth = 17cm, top = 2cm, bottom = 2cm]{geometry}
\usepackage{nameref}
\usepackage{bm}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}

\newcommand{\pkg}[1]{\texorpdfstring%
{{\normalfont\fontseries{b}\selectfont #1}}%
{#1}}

\begin{document}
\SweaveOpts{engine = R}
%\VignetteIndexEntry{rebmix: The Rebmix Package}
%\VignetteKeyword{continuous variable}
%\VignetteKeyword{discrete variable}
%\VignetteKeyword{finite mixture}
%\VignetteKeyword{parameter estimation}
%\VignetteKeyword{REBMIX algorithm}
%\VignettePackage{rebmix}
\bibliographystyle{abbrvnat}
\title{\pkg{rebmix}: The Rebmix Package}
\author{Marko Nagode}
\date{\today}
\maketitle
\begin{abstract}
The \pkg{rebmix} package for R provides functions for random univariate and multivariate finite mixture generation, number of components, component weights and component parameter estimation, bootstrapping and plotting of the finite mixtures. It relies on the REBMIX algorithm that requires preprocessing, information criterion and conditionally independent normal, lognormal, Weibull, gamma, binomial, Poisson or Dirac component densities. The algorithm optimizes the component parameters, mixing weights and number of components successively based on the boundary conditions, such as the maximum number of components, total of positive relative deviations, number of classes or nearest neighbours. The algorithm is robust, time efficient and can be used either to assess the initial set of the unknown parameters and number of components for, e.g., the EM algorithm or as a standalone algorithm that is a good compromise between the nonparametric and parametric methods to the finite mixture estimation.
\end{abstract}
\section{Introduction}\label{sec:introduction}
Finite mixture models are used increasingly to model the distributions of a wide variety of random phenomena. For the multivariate data of continuous nature, attention is paid to the use of multivariate normal components because of their computational convenience \citep{McLachlan_1999, Ingrassia_Rocci_2007, Fruhwirth_Schnatter_2006}. However, in fatigue and reliability analyses, lognormal and Weibull distributions are preferred due to their flexibility and their definition for continuous positive random variables only \citep{Majeske_2003, Sultan_2007, Touw_2009}.

The finite mixture models have seen a real boost in popularity over the last two decades due to the tremendous increase in available computing power. These models can be applied to data where observations originate from various groups and the group affiliations are not known, and on the other hand to provide approximations for multimodal distributions \citet{Leisch_2004}. Some of the latest models can be found also in \citet{McLachlan_2013, Chavent_2012, Grun_2012, Melnykov_2012, Dijk_2009, Benaglia_2009, Grun_Leisch_2008, Fraley_2007, McLachlan_and_Peel_2000}.

The REBMIX algorithm origins in \citet{Nagode_1998} and avoids the drawbacks of the EM algorithm:
\begin{itemize}
\item The EM algorithm converges to a local maximum of the likelihood function very quickly.
\item There are often several other promising local optimal solutions in the vicinity of the solutions obtained from methods that provide good initial guesses of the solution.
\item Model selection criterion usually assumes that the global optimal solution of the log-likelihood function can be obtained. However, achieving this is computationally intractable.
\item Some regions in the search space do not contain any promising solutions. The promising and non-promising regions
co-exist, and it often becomes challenging to avoid wasting computational resources to search in non-promising regions.
\end{itemize}\citet{}
reported in \citet{Reddy_Rajaratnam_2010} by updating the number of components, component weights and component parameters sequentially and not simultaneously \citep[see also][]{Celeux_2001}. Later on the REBMIX has evolved \citep{Nagode_2000, Nagode_2001, Nagode_2006, Nagode_Fajdiga_2011a, Nagode_Fajdiga_2011b}. The paper extends it to discrete variables by adding binomial, Poisson and Dirac parametric families. Gamma parametric family is added as well. REBMIX stands for a robust, time efficient tool that can be used either to assess the initial set of unknown parameters and the number of components for other algorithms \citep{Chen_2013, Bucar_2004} or as a standalone procedure that is a good compromise between the nonparametric and parametric methods to the finite mixture estimation.

The \pkg{rebmix} implementation of the REBMIX \citep{rebmix_2014} extends the set of algorithms available for random univariate and multivariate finite mixture generation, number of components, component weights and component parameter estimation, bootstrapping and plotting of the finite mixtures in the R language and environment for statistical computing.

The outline of the paper is as follows: Section~\nameref{sec:algorithm} presents the algorithm. Section~\nameref{sec:examples} analyses the performance of the approach by studying four datasets. Section~\nameref{sec:summary} lists the conclusions.
\section{Algorithm}\label{sec:algorithm}
Let $\bm{y}_{1}, \ldots, \bm{y}_{n}$ be an observed $d$~dimensional dataset of size $n$ of continuous or discrete
vector observations $\bm{y}_{j}$. Each observation is assumed to follow predictive mixture density
\begin{equation}
f(\bm{y} | c, \bm{w}, \bm{\Theta}) = \sum_{l = 1}^{c} w_{l} f(\bm{y} | \bm{\theta}_{l})
\end{equation}
with conditionally independent component densities
\begin{equation}\label{eq:3}
f(\bm{y} | \bm{\theta}_{l}) = \prod_{i = 1}^{d} f(y_{i} | \bm{\theta}_{il})
\end{equation}
indexed by vector parameter $\bm{\theta}_{l} = (\bm{\theta}_{1l}, \ldots, \bm{\theta}_{dl})^{\top}$. The components can currently belong to either normal
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \frac{1}{\sqrt{2 \pi} \sigma_{il}} \exp \left \{-\frac{1}{2} \frac{(y_{i} - \mu_{il})^{2}}{\sigma_{il}^{2}}\right \} \nonumber,
\end{equation}
lognornal
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \frac{1}{\sqrt{2 \pi} \sigma_{il} y_{i}} \exp \left \{-\frac{1}{2} \frac{(\log(y_{i}) - \mu_{il})^{2}}{\sigma_{il}^{2}}\right \} \nonumber,
\end{equation}
Weibull
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \frac{\beta_{il}}{\theta_{il}} \left (\frac{y_{i}}{\theta_{il}} \right )^{\beta_{il} - 1} \exp \left \{-\left (\frac{y_{i}}{\theta_{il}} \right )^{\beta_{il}} \right \} \nonumber,
\end{equation}
gamma
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \frac{1}{\Gamma [\beta_{il}] y_{i}} \left (\frac{y_{i}}{\theta_{il}} \right )^{\beta_{il}} \exp \left \{ -\frac{y_{i}}{\theta_{il}} \right \} \nonumber,
\end{equation}
binomial
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \binom{\theta_{il}}{y_{i}}p_{il}^{y_{i}}(1 - p_{il})^{\theta_{il} - y_{i}} \nonumber,
\end{equation}
Poisson
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \frac{e^{-\theta_{il}} \theta_{il}^{y_{i}}}{y_{i}!} \nonumber
\end{equation}
or Dirac
\begin{equation}
f(y_{i} | \bm{\theta}_{il}) = \left\{ \begin{array}{l l} 1 & y_{i} = \theta_{il}\\ 0 & \textrm{otherwise} \end{array} \right. \nonumber
\end{equation}
parametric family types. The objective of the analysis is the inference about the number $c$ of components, component weights $w_{l}$ summing to 1 and component parameters $\bm{\theta}_{l}$. The REBMIX algorithm is an iterative numerical procedure relying on the suppositions:
\begin{itemize}
\item It is always possible to assign empirical densities to an arbitrary dataset.
\item Based on the empirical densities, global mode position can be identified.
\item Once the global mode position and its empirical density are known, rough component parameters of the predictive component density can be estimated.
\item Based on the rough component parameters, the dataset can be clustered successively into the classes linked to the predictive component densities and the residue.
\item The number $c$ of components equals the number of the classes.
\item Enhanced component parameters and the component weights can be assessed for all classes.
\item The remaining observations can be distributed between the existing components by the Bayes decision rule and the parameters of the finite mixture can be fine-tuned.
\end{itemize}
Sections~\nameref{subsec:preprocessing_of_observations} to \nameref{subsec:bayes_classification_of_the_remaining_observations} give the theoretical backgrounds for the algorithm, while Section~\nameref{subsec:algorithm_flow} lists and explains its flow.
\subsection{Preprocessing of observations}\label{subsec:preprocessing_of_observations}
The algorithm requires the preprocessing of observations. By the histogram approach, the dataset is counted into a finite number of nonoverlapping, equally sized and regularly distributed bins. Assuming that bin means $\bar{\bm{y}}_{j} = (\bar{y}_{1j}, \ldots, \bar{y}_{dj})^{\top}$ are given by
\begin{equation}
\bar{y}_{ij} = \bar{y}_{i0} + \textrm{'An arbitrary integer'} \times h_{ij}, \ i = 1, \ldots, d,
\end{equation}
the fraction of observations $k_{j}$ for $j = 1, \ldots, v$ falling into volume $V_{j}$ is counted out, where $\bar{y}_{i0}$ stands for an arbitrary origin and $v$ depicts the number of nonempty bins. Similarly, if the Parzen window is employed, the fraction of observations falling into $V_{j}$ centered on observation $\bm{y}_{j}$ is obtained. In both cases, the volume is taken to be a hypersquare with the sides of length $h_{ij}$. This yields $V_{j} = \prod_{i = 1}^{d} h_{ij}$. Moreover, for both approaches class widths $h_{ij} = h_{i}$ and volumes $V_{j} = V$ are kept constant. If the $k$-nearest neighbour approach is used, the fraction of observations falling into normalized hypersphere $V_{j} = \pi^{d/2} R_{j}^{d} / \Gamma [1 + d / 2]$ of radius $R_{j}$ centered on observation $\bm{y}_{j}$ contains constant number $k_{j} = k$ of observations. The class widths for the histogram and Parzen window approach and continuous parametric families
\begin{eqnarray}
h_{i} = \frac{y_{i\mathrm{max}} - y_{i\mathrm{min}}}{v} \nonumber
\end{eqnarray}
depend on the minimum $y_{i\mathrm{min}} = \operatorname{min} {y}_{ij}$ and maximum $y_{i\mathrm{max}} = \operatorname{max} {y}_{ij}$ observations. For the histogram approach and continuous parametric families origin is preset to
\begin{eqnarray}
\bar{y}_{i0} = y_{i\mathrm{min}} + \frac{h_{i}}{2}. \nonumber
\end{eqnarray}
However, discrete parametric families require $h_{i} = 1$ and $\bar{y}_{i0} = y_{i\mathrm{min}}$. The $k\textrm{th} - 1$ nearest neighbour $\bm{y}_{\hat{j}}$ is searched around $\bm{y}_{j}$ based on the normalized Euclidean distance
\begin{eqnarray}
R_{j} = \sqrt{\sum_{i=1}^{d}\left( \frac{y_{i\hat{j}} - y_{ij}}{y_{i\mathrm{max}} - y_{i\mathrm{min}}} \right)^{2}} \textrm{ for } \hat{j} \neq j \textrm{ and } h_{ij} = 2 R_{j} (y_{i\mathrm{max}} - y_{i\mathrm{min}}). \nonumber
\end{eqnarray}
If $N \geq k$ nearest neighbours coincide, then the normalized Euclidean distance $R_{j}$ to the first nearest non-coincident neighbour $\bm{y}_{\hat{j}}$ is multiplied by $(k / (N + 1))^{1 / d}$. Infinite empirical density estimations are thus prevented.
\subsection{Global mode detection}\label{subsec:global_mode_detection}
Argument $m$ at which empirical density $f_{lj}$
\begin{equation}
m = \underset{j}{\operatorname{arg} \operatorname{max}} f_{lj}
\end{equation}
attains its maximum determines the global mode. If observations are binned into the histogram, then
\begin{equation}
f_{lj} = \frac{k_{lj}}{n_{l}} \frac{1}{V_{j}}, \ j = 1, \ldots, v,
\end{equation}
where frequencies $k_{lj}$ are all set to $k_{j}$ initially and number of observations in class $l$ is
\begin{eqnarray}
n_{l} = \sum_{j = 1}^{v} k_{lj}. \nonumber
\end{eqnarray}
If the Parzen window or $k$-nearest neighbour approach is applied,
\begin{equation}
f_{lj} = \frac{k_{lj}}{n_{l}} \frac{k_{j}}{V_{j}}, \ j = 1, \ldots, n.
\end{equation}
Frequencies $k_{lj}$ are all set to $1$ initially, $n_{l} = \sum_{j = 1}^{n} k_{lj}$ and component weight $w_{l} = n_{l} / n$. Moreover, the $l$th component conditional empirical density at the global mode for the histogram approach
\begin{equation}
f_{i | \hat{i}.lm} = \frac{k_{lm}}{\sum_{j = 1,\; \bar{y}_{\hat{i}j} = \bar{y}_{\hat{i}m}}^{v} k_{lj}} \frac{1}{h_{im}} = \frac{k_{lm}}{k_{i | \hat{i}.lm}} \frac{1}{h_{im}}
\end{equation}
is required, where index $\hat{i} = 1, \ldots, i - 1, i + 1, \ldots, d$. If $d = 1$, then $k_{i | \hat{i}.lm} = n_{l}$ and $f_{i | \hat{i}.lm} = f_{lm}$.
For the Parzen window and $k$-nearest neighbour approach
\begin{equation}
f_{i | \hat{i}.lm} = \frac{k_{lm}}{\sum_{j = 1,\; |y_{\hat{i}j} - y_{\hat{i}m}| \leq h_{\hat{i}m} / 2}^{n} k_{lj}} \frac{k_{m}}{h_{im}} = \frac{k_{lm}}{k_{i | \hat{i}.lm}} \frac{k_{m}}{h_{im}}.
\end{equation}
\subsection{Clustering of observations}\label{subsec:clustering_of_observations}
The clustering of observations is an iterative procedure of identifying the observations belonging to the $l$th component. The deviations between $k_{lj}$ and the predictive component frequencies for the histogram approach are given by
\begin{equation}
e_{lj} = k_{lj} - n_{l} f(\bar{\bm{y}}_{j} | \bm{\theta}_{l}) V_{j}.
\end{equation}
For the Parzen window and $k$-nearest neighbour approach
\begin{equation}\label{eq:7}
e_{lj} = k_{lj} - n_{l} f(\bm{y}_{j} | \bm{\theta}_{l}) V_{j} / k_{j}.
\end{equation}
To identify the most deviating observations, relative positive deviations $\varepsilon_{lj} = e_{lj} / k_{lj}$ and maximum positive relative deviation $\varepsilon_{l\mathrm{max}}$ are calculated. Total of positive and negative deviations
\begin{eqnarray}
e_{l\mathrm{p}} = \sum_{j = 1,\; e_{lj} > 0}^{v} e_{lj} \textrm{ and } e_{l\mathrm{n}} = \sum_{j = 1,\; e_{lj} < 0}^{v} \operatorname{max} \{e_{lj}, -r_{j}\}, \nonumber
\end{eqnarray}
where $r_{j}$ stands for the residual frequency. If index $v$ is replaced by $n$ the equation can be used with the Parzen window and $k$-nearest neighbour approach, too. Total of positive relative deviations of the $l$th component is then
\begin{equation}
D_{l} = \frac{e_{l\mathrm{p}}}{n_{l}},
\end{equation}
where $0 \leq D_{l} \leq 1$. The observations that inequality $\varepsilon_{lj} > \varepsilon_{l\mathrm{max}} (1 - a_{\mathrm{r}})$ holds for are not assumed to belong
to the $l$th component and therefore move to the residue. Number of iterations depends on acceleration rate $0 < a_{\mathrm{r}} \leq 1$. It is best to keep
$a_{\mathrm{r}}$ close to zero. The recommended value is $0.1$. On the contrary, the observations where $e_{lj} < 0$ are transferred back to the $l$th component. The clustering of observations continues with the renewed rough parameter and component weight estimation until
\begin{equation}
D_{l} \leq \frac{D_{\mathrm{min}}}{w_{l}}.
\end{equation}
Constant $0 < D_{\mathrm{min}} \leq 1$ is optimized by the information criterion. The clustering of observations ends with the enhanced component parameter estimation.
\subsection{Rough component parameter estimation}\label{subsec:rough_component_parameter_estimation}
The clustering of observations depends on the rough component parameters. Proper extraction of observations belonging to the $l$th component is assured by the restraints that prevent the component from its flowing away from the global mode as at least one component is supposed to be in its vicinity. This yields
\begin{equation}\label{eq:15}
f(\bm{y} = \hat{\bm{y}}_{m} | \bm{\theta}_{l}) = f_{lm},
\end{equation}
where $\hat{\bm{y}}_{m} = \bar{\bm{y}}_{m}$ for the histogram and $\hat{\bm{y}}_{m} = \bm{y}_{m}$ for the Parzen window and $k$-nearest neighbour approach. Restraint (\ref{eq:15}) is insufficient if $d > 1$ even for single parameter component densities, such as for Dirac and exponential. Allowing for the independence of components (\ref{eq:3}) equation (\ref{eq:15}) yields
\begin{eqnarray}
\prod_{i = 1}^{d} f(y_{i} = \hat{y}_{im} | \bm{\theta}_{il}) = f_{lm} = \prod_{i = 1}^{d} \varepsilon f_{i | \hat{i} . lm},
\end{eqnarray}
wherefrom required restraints
\begin{equation}\label{eq:4}
f(y_{i} = \hat{y}_{im} | \bm{\theta}_{il}) = \varepsilon f_{i | \hat{i} . lm} =  f_{i | \hat{i} . l\mathrm{max}}, \ i = 1, \ldots, d
\end{equation}
can be derived. In addition, from known $f_{lm}$ and $f_{i | \hat{i} . lm}$ it follows
\begin{equation}
\varepsilon = \mathrm{min} \left \{1, \left (\frac{f_{lm}}{\prod_{i = 1}^{d} f_{i | \hat{i} . lm}} \right )^{\frac{1}{d}} \right \},
\end{equation}
where the upper limit of $\varepsilon$ is set to $1$. For Rayleigh, Poisson or binomial distribution with known $\theta_{il}$ it is assumed
\begin{equation}\label{eq:2}
\frac{\partial f(y_{i} = \hat{y}_{im} | \bm{\theta}_{il})}{\partial y_{i}} = 0, \ i = 1, \ldots, d.
\end{equation}
The rough component parameters for single parameter distributions are thus gained from (\ref{eq:4}) or (\ref{eq:2}). For two parameter normal, lognormal, Weibull or gamma distribution Lagrange multiplier
\begin{equation}\label{eq:1}
\Lambda (\bm{\theta}_{il}, \lambda_{il}) = -\int_{-\infty}^{+\infty} f(y_{i} | \bm{\theta}_{il}) \log (f(y_{i} | \bm{\theta}_{il})) dy_{i} + \lambda_{il} \log (f(y_{i} = \hat{y}_{im} | \bm{\theta}_{il}) / f_{i | \hat{i} . l\mathrm{max}})
\end{equation}
provides a strategy for entropy maximization subject to logarithm of (\ref{eq:4}). The rough component parameters for two parameter distributions are then a solution of
\begin{equation}\label{eq:11}
\nabla_{\bm{\theta}_{il}, \lambda_{il}} \Lambda (\bm{\theta}_{il}, \lambda_{il}) = 0, \ i = 1, \ldots, d.
\end{equation}
Constrained entropy (\ref{eq:1}) maximization enables rough Weibull and gamma parameter estimation for shape parameter $\beta_{il} > 0$ and not only for $\beta_{il} > 1$ as in \citet{Nagode_Fajdiga_2011a, Nagode_Fajdiga_2011b}. Rough normal component parameters are given by
\begin{equation}\label{eq:5}
\mu_{il} = \hat{y}_{im} \textrm{ and } \sigma_{il} = \frac{1}{\sqrt{2 \pi} f_{i | \hat{i} . l\mathrm{max}}}.
\end{equation}
Similarly, rough lognormal
\begin{multline}
f(\lambda_{il}) = \frac{\lambda_{il} - 1}{\lambda_{il}} + \log (\lambda_{il} (\lambda_{il} - 1)) + 2 \log (\sqrt{2 \pi} f_{i | \hat{i} . l\mathrm{max}} \hat{y}_{im}) = 0 \textrm{, } \\ \mu_{il} = \lambda_{il} - 1 + \log(\hat{y}_{im}) \textrm{ and } \sigma_{il} = \sqrt{\lambda_{il} (\lambda_{il} - 1)},
\end{multline}
Weibull
\begin{multline}\label{eq:12}
f(\alpha_{il}) = \frac{\alpha_{il} - 1}{\lambda_{il}} e^{\frac{1}{\alpha_{il}}} - f_{i | \hat{i} . l\mathrm{max}} \hat{y}_{im} e = 0 \textrm{, } \lambda_{il} = \frac{\alpha_{il}}{\beta_{il}} \textrm{, } \\
\beta_{il} = \alpha_{il} + \gamma + \log \left (\frac{\alpha_{il} - 1}{\alpha_{il}} \right) \textrm{, } \theta_{il} = \hat{y}_{im} \left ( \frac{\alpha_{il}}{\alpha_{il} - 1} \right)^{\frac{1}{\beta_{il}}} \textrm{ and } \beta_{il} > 0,
\end{multline}
gamma
\begin{multline}\label{eq:13}
f(\alpha_{il}) = \frac{1}{2} \log (\beta_{il}) + \beta_{il} \left (\log \left (\frac{\alpha_{il} - 1}{\alpha_{il}} \right) + \frac{1}{\alpha_{il}} \right ) - \log (\sqrt{2 \pi} f_{i | \hat{i} . l\mathrm{max}} \hat{y}_{im}) = 0 \textrm{, } \\
\beta_{il} = \frac{\gamma (1 + \alpha_{il})}{\gamma - 1 - \alpha_{il} \log \left (\frac{\alpha_{il} - 1}{\alpha_{il}} \right)} \textrm{, } \lambda_{il} = \frac{\alpha_{il}}{\beta_{il}} \textrm{, } \theta_{il} = \frac{\hat{y}_{im} \lambda_{il}}{\alpha_{il} - 1} \textrm{ and } \beta_{il} > 0,
\end{multline}
binomial
\begin{equation}
p_{il} = \left\{ \begin{array}{l l} 1 - f_{i | \hat{i} . l\mathrm{max}}^{1 / \theta_{il}} & \hat{y}_{im} = 0 \\ f_{i | \hat{i} . l\mathrm{max}}^{1 / \theta_{il}} & \hat{y}_{im} = \theta_{il} \\ \hat{y}_{im} / \theta_{il} & \textrm{otherwise}, \end{array} \right.
\end{equation}
rough Poisson
\begin{equation}
\theta_{il} = \left\{ \begin{array}{l l} - \operatorname{log}(f_{i | \hat{i} . l\mathrm{max}}) & \hat{y}_{im} = 0 \\ \hat{y}_{im} & \textrm{otherwise} \\ \end{array} \right.
\end{equation}
and rough Dirac
\begin{equation}\label{eq:8}
\theta_{il} = \hat{y}_{im}
\end{equation}
component parameters are derived, where $\gamma$ is the Euler-Mascheroni constant. When deriving (\ref{eq:13}) $\Gamma [\beta_{il}]$ is approximated by the Stirling's formula and digamma function by $\psi(\beta_{il}) = \log(\beta_{il}) - \gamma / \beta_{il}$. Rough binomial parameter $\theta_{il} = \theta_{i}$ is fixed and equals the number of categories minus one.

The rigid restraints result in poor component parameter estimation if modes of several component densities coincide. The loose restraints introduced in \cite{Nagode_Fajdiga_2011a} improve component parameter estimation and offer further evolution opportunities. The rigid restraints become loose if $f_{i | \hat{i} . l \mathrm{max}}$ in equations (\ref{eq:5}) to (\ref{eq:8}) is replaced by $f_{i | \hat{i} . lm}$, where
\begin{equation}
0 \leq f_{i | \hat{i} . lm} \leq f_{i | \hat{i} . l \mathrm{max}}.
\end{equation}
Instead of minimizing the maximum relative positive deviation \citep{Nagode_Fajdiga_2011a} the simpler root finding of the total of relative deviations is used here to attain the optimal $f_{i | \hat{i} . lm}$. For the histogram approach total of relative deviations
\begin{equation}
D_{i | \hat{i} . lm} =  1 - \sum_{j = 1,\; \bar{y}_{\hat{i}j} = \bar{y}_{\hat{i}m}}^{v} f(y_{i} = \bar{y}_{ij} | \bm{\theta}_{il}) h_{ij} \end{equation}
equals the fraction of observations falling into the regions on $y_{i}$ axis with zero empirical probability. If $D_{i | \hat{i} . lm}$ is close to zero, e.g., $0.002$, then observations not contributing significantly to the $l$th component should not affect the loose component parameter estimation. This yields
\begin{equation}\label{eq:14}
\sum_{j = 1,\; \bar{y}_{\hat{i}j} = \bar{y}_{\hat{i}m}}^{v} f(\bar{y}_{ij} | \bm{\theta}_{il}) h_{ij} =  0.998
\end{equation}
Equation (\ref{eq:14}) can be solved for optimal $f_{i | \hat{i} . lm}$ by the bisection root finding method. If the root does not exist, then $f_{i | \hat{i} . lm} = f_{i | \hat{i} . l \mathrm{max}}$.
For the Parzen window and $k$-nearest neighbour approach the root of
\begin{equation}
\sum_{j = 1,\; |y_{\hat{i}j} - y_{\hat{i}m}| \leq h_{\hat{i}m} / 2}^{n} f(y_{ij} | \bm{\theta}_{il}) h_{ij} / k_{j} = 0.998
\end{equation}
is searched for optimal $f_{i | \hat{i} . lm}$. Dirac parameter $\theta_{il}$ of (\ref{eq:8}) does not require $f_{i | \hat{i} . lm}$ optimization.
\subsection{Enhanced component parameter estimation}\label{subsec:enhanced_component_parameter_estimation}
Maximum likelihood is employed to get enhanced component parameters. For the histogram approach enhanced normal component parameters are given
by \begin{equation}
\mu_{il} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij} \textrm{ and } \sigma_{il}^{2} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij}^{2} - \mu_{il}^{2}.
\end{equation}
Likewise, enhanced lognormal
\begin{equation}
\mu_{il} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \log(\hat{y}_{ij}) \textrm{ and } \sigma_{il}^{2} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \log(\hat{y}_{ij})^{2} - \mu_{il}^{2},
\end{equation}
Weibull
\begin{equation}
\theta_{il}^{\beta_{il}} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij}^{\beta_{il}} \textrm{ and } f(\beta_{il}) = \frac{1}{\beta_{il}} + \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \log(\hat{y}_{ij}) - \frac{\sum_{j = 1}^{v} k_{lj} \hat{y}_{ij}^{\beta_{il}} \log(\hat{y}_{ij})}{\sum_{j = 1}^{v} k_{lj} \hat{y}_{ij}^{\beta_{il}}} = 0,
\end{equation}
gamma
\begin{equation}
\theta_{il} = \frac{1}{\beta_{il} n_{l}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij} \textrm{ and } f(\beta_{il}) = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \log (\hat{y}_{ij}) - \log (\theta_{il}) - \frac{\Gamma' [\beta_{il}]}{\Gamma [\beta_{il}]} = 0,
\end{equation}
binomial
\begin{equation}
p_{il} = \frac{1}{n_{l} \theta_{il}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij},
\end{equation}
Poisson
\begin{equation}
\theta_{il} = \frac{1}{n_{l}} \sum_{j = 1}^{v} k_{lj} \hat{y}_{ij}
\end{equation}
and Dirac component parameters
\begin{equation}\label{eq:10}
\theta_{il} = \hat{y}_{im}
\end{equation}
are estimated. Index $v$ is replaced by $n$ for the Parzen window or $k$-nearest neighbour approach.
\subsection{First and second moment calculation}\label{subsec:first_and_second_moment_calculation}
The first and second moment of the normal
\begin{equation}\label{eq:6}
m_{il} = \mu_{il} \textrm{ and } V_{il} = \sigma_{il}^{2} + \mu_{il}^{2},
\end{equation}
lognormal
\begin{equation}
m_{il} = e^{ \mu_{il} + \frac{\sigma_{il}^{2}}{2} } \textrm{ and } V_{il} = e^{ 2 \mu_{il} + 2 \sigma_{il}^{2} },
\end{equation}
Weibull
\begin{equation}
m_{il} = \theta_{il} \Gamma \left [ 1 + \frac{1}{\beta_{il}} \right ] \textrm{ and } V_{il} = \theta_{il}^{2} \Gamma \left [ 1 + \frac{2}{\beta_{il}}
\right ],
\end{equation}
gamma
\begin{equation}
m_{il} = \theta_{il} \beta_{il} \textrm{ and } V_{il} = \theta_{il}^{2} \beta_{il} (1 + \beta_{il})
\end{equation}
and the first moment of binomial
\begin{equation}
m_{il} = \theta_{il} p_{il},
\end{equation}
Poisson
\begin{equation}
m_{il} = \theta_{il}
\end{equation}
and Dirac
\begin{equation}\label{eq:9}
m_{il} = \theta_{il}
\end{equation}
distributions are calculated to enable the classification of the remaining observations.
\subsection{Bayes classification of the remaining observations}\label{subsec:bayes_classification_of_the_remaining_observations}
With the increase of the number of components, the number $n_{l}$ of the remaining observations decreases. When the component weight attains the minimum weight \begin{equation}
w_{l} \leq w_{\mathrm{min}} = D_{\mathrm{min}} (l - 1)
\end{equation}
it is assumed that remaining observations $k_{lj}$ belong to the existing classes and do not form the new ones. The classification of the remaining observations is accomplished by the Bayes decision rule \citep{Duda_and_Hart_1973}
\begin{gather}
l =  \underset{l}{\operatorname{arg} \operatorname{max}} w_{l} f(\bm{y}_{j} | \bm{\theta}_{l}) \nonumber \\ w_{l} = w_{l} + \frac{k_{lj}}{n} \textrm{, } m_{il} = m_{il} + \frac{k_{lj} (y_{ij} - m_{il})}{n w_{l}} \textrm{ and } V_{il} = V_{il} + \frac{k_{lj} (y_{ij}^{2} - V_{il})}{n w_{l}},
\end{gather}
where $k_{lj}$ is added to the $l$th class and the component weight and both moments are recalculated \citep{Bishop_1995}. Once all $v$ bin means or all
$n$ observations are processed, the predictive mixture parameters are gained by inverting (\ref{eq:6}) to (\ref{eq:9}).
\subsection{Algorithm flow}\label{subsec:algorithm_flow}
\begin{figure}[htbp]\centering
\includegraphics[width = \textwidth]{algorithm}
\caption{REBMIX algorithm.}\label{figure:algorithm}
\end{figure}
The REBMIX is listed in Figure~\ref{figure:algorithm}. It requires fourteen arguments, whereby depending on the parametric families five or six of them are mandatory, the rest is optional. It consists of three main loops: the inner $9 \rightarrow 37$, the middle $6 \rightarrow 41$ and the outer loop $4 \rightarrow 47$. The numbers are line indices. In line 2 the observations are preprocessed as described in Section~\nameref{subsec:preprocessing_of_observations}. In line 3, counter $I_{1}$, constant $D_{\mathrm{min}}$ and frequencies $k_{lj}$ are initiated. Next, the outer loop begins. Line 5 presumes that the mixture consists of one component, then the number $r$ of observations to separate is set to $n$ and $n_{l}$ to $n$. If ratio $n_{l} / n$ is greater than the minimum weight introduced in Section~\nameref{subsec:bayes_classification_of_the_remaining_observations}, the middle loop enters. Otherwise, the finite mixture parameter estimation for $v \in K$ is completed.

In lines 7 and 8, global mode argument $m$ is detected as explained in Section~\nameref{subsec:global_mode_detection}, counter $I_{2}$ is initiated, component weight $w_{l}$ is calculated and frequencies $r_{j}$ are all set to zero. If $I_{2} \leq I_{\mathrm{max}}$, the inner loop enters, otherwise in line 38 the first and the second moments are calculated (see Section~\nameref{subsec:first_and_second_moment_calculation}). Next, number of components $c$ is set to $l$, number of observations $r$ is decreased by $n_{l}$, $l$ is incremented, number $r$ of the remaining observations joins $n_{l}$, residue frequencies $r_{j}$ are all moved to $k_{lj}$, and the Stop criterion is determined.

The inner loop is divided into three sections. In line 10 the component parameters are estimated roughly (see Section~\nameref{subsec:rough_component_parameter_estimation}). In the second section $11 \rightarrow 23$, total of positive relative deviations $D_{l}$ and maximum relative deviation $\varepsilon_{l\mathrm{max}}$ are calculated. The number of iterations depends on acceleration rate $a_\mathrm{r}$. In the third section $24 \rightarrow 35$, the maximum and negative deviations are transferred between frequencies $k_{lj}$ and residue $r_{j}$. This way deviations $e_{lj}$ are reduced gradually. The negative value of $e_{lj}$ can never be higher than residue value $r_{j}$. If this is not true, deviation $e_{lj}$ is corrected as listed in line 19. When the condition in line 24 is not fulfilled, the enhanced component parameter estimation is carried out (see Section~\nameref{subsec:enhanced_component_parameter_estimation}) and the inner loop ends.

The enhanced component parameter estimation may fail. In this instance, the component parameters are reset to the state just before the failure occurred. In line 42 the remaining observations are classified by the Bayes decision rule as depicted in Section~\nameref{subsec:bayes_classification_of_the_remaining_observations}. Further on, information criterion, e.g., \citet{Akaike_1974}
\begin{equation}
\mathrm{IC} = -2 \operatorname{log} L(c, \bm{w}, \bm{\Theta}) + 2 M
\end{equation}
is calculated, whereas the number of free parameters for the normal, lognormal, Weibull and gamma mixtures can be written as
\begin{equation}
M = 2 c d + c - 1.
\end{equation}
The binomial, Poisson and Dirac mixtures require $M = c d + c - 1$. The log likelihood function for the binned observations is given by
\begin{equation}
\operatorname{log} L(c, \bm{w}, \bm{\Theta}) = \sum_{j = 1}^{v} k_{j} \operatorname{log} f(\bar{\bm{y}}_{j} | c, \bm{w}, \bm{\Theta}).
\end{equation}
Otherwise,
\begin{equation}
\operatorname{log} L(c, \bm{w}, \bm{\Theta}) = \sum_{j = 1}^{n} \operatorname{log} f(\bm{y}_{j} | c, \bm{w}, \bm{\Theta}).
\end{equation}
Finally, total of positive relative deviations for the histogram
\begin{equation}
D = \sum_{j = 1}^{v} \left \langle \frac{k_{j}}{n} - f(\bar{\bm{y}}_{j} | c, \bm{w}, \bm{\Theta}) V_{j} \right \rangle,
\end{equation}
Parzen window or $k$-nearest neighbour
\begin{equation}
D = \sum_{j = 1}^{n} \left \langle \frac{1}{n} - \frac{f(\bm{y}_{j} | c, \bm{w}, \bm{\Theta}) V_{j}}{k_{j}} \right \rangle
\end{equation}
is calculated, where $\langle x \rangle = x$ if $x > 0$ and $\langle x \rangle = 0$ if $x \leq 0$. This way global optimum $\mathrm{IC_{opt}}$ corresponding to the optimal number $c_{\mathrm{opt}}$ of components, weights $\bm{w}_{\mathrm{opt}}$ and parameters $\bm{\Theta}_{\mathrm{opt}}$ can always be found. In line 46, $D_{\mathrm{min}}$ is decreased in such a way that total of positive relative deviations
\begin{eqnarray}
c D_{{\mathrm{min}}}^{{\mathrm{old}}} = (c + 1) D_{{\mathrm{min}}}^{{\mathrm{new}}} \nonumber
\end{eqnarray}
for $c$ and $c + 1$ components is preserved. When line 47 is fulfilled, the procedure stops. If index $v$ in Figure~\ref{figure:algorithm} is replaced by $n$ and line 15 is replaced by (\ref{eq:7}) the algorithm, presented for the histogram approach, can also be used with the Parzen window and $k$-nearest neighbour approach.
\section{Examples}\label{sec:examples}
To illustrate the use of the REBMIX algorithm, univariate and multivariate datasets are considered. The \pkg{rebmix} is loaded and the prompt before starting new page is set to \texttt{TRUE}.
<<rebmix-code, split = FALSE, echo = FALSE, keep.source = FALSE>>=
##############################################
## R sources for reproducing the results in ##
##   Marko Nagode:                          ##
##   rebmix: The Rebmix Package             ##
##############################################

options(prompt = "R> ", continue = "+  ", width = 80,
  useFancyQuotes = FALSE, digits = 3)
@
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
###################
## Preliminaries ##
###################

## load package and set prompt before starting new page to TRUE.

library("rebmix")
devAskNewPage(ask = TRUE)
@
\subsection{Gamma datasets}\label{subsec:gamma_datasets}
Three gamma mixtures are considered \citep{Wiper_2001}. The first has four well-separated components with means $2$, $4$, $6$ and $8$, respectively
\begin{center}
\(\begin{array}{lll}
\theta_{1} = 1/100 & \beta_{1} = 200 & n_{1} = 100 \\
\theta_{2} = 1/100 & \beta_{2} = 400 & n_{2} = 100 \\
\theta_{3} = 1/100 & \beta_{3} = 600 & n_{3} = 100 \\
\theta_{4} = 1/100 & \beta_{4} = 800 & n_{4} = 100.
\end{array}\)
\end{center}
The second has equal means but different variances and weights
\begin{center}
\(\begin{array}{lll}
\theta_{1} = 1/27 & \beta_{1} = 9 & n_{1} = 40 \\
\theta_{2} = 1/270 & \beta_{2} = 90 & n_{2} = 360.
\end{array}\)
\end{center}
The third is a mixture of a rather diffuse component with mean $6$ and two lower weighted components with smaller variances and means of $2$ and $10$, respectively
\begin{center}
\(\begin{array}{lll}
\theta_{1} = 1/20 & \beta_{1} = 40 & n_{1} = 80 \\
\theta_{2} = 1 & \beta_{2} = 6 & n_{2} = 240 \\
\theta_{3} = 1/20 & \beta_{3} = 200 & n_{3} = 80.
\end{array}\)
\end{center}
The gamma mixtures are generated by calling the \texttt{RNGMIX} function. It demands character vector \texttt{Dataset} containing list names of data frames that datasets are written in, random seed \texttt{rseed}, vector \texttt{n} containing number of observations in classes $n_{l}$ and a matrix containing $c$ parametric family types \texttt{pdfi}. One of \texttt{"normal"}, \texttt{"lognormal"}, \texttt{"Weibull"}, \texttt{"gamma"}, \texttt{"binomial"}, \texttt{"Poisson"} or \texttt{"Dirac"}. Component parameters \texttt{theta1.i} follow the parametric family types. One of $\mu_{il}$ for normal and lognormal distributions and $\theta_{il}$ for Weibull, gamma, binomial, Poisson and Dirac distributions. Component parameters \texttt{theta2.i} follow \texttt{theta1.i}. One of $\sigma_{il}$ for normal and lognormal distributions, $\beta_{il}$ for Weibull and gamma distributions and $p_{il}$ for binomial distribution.
<<rebmix-code, split = FALSE, results = hide, keep.source = FALSE>>=
######################
##  Gamma datasets  ##
######################

## Generate gamma datasets.

n <- c(100, 100, 100, 100)

Theta <- rbind(pdf = "gamma",
  theta1 = c(1/100, 1/100, 1/100, 1/100),
  theta2 = c(200, 400, 600, 800))

gamma1 <- RNGMIX(Dataset = "gamma1", n = n, Theta = Theta)

n <- c(40, 360)

Theta <- rbind(pdf = "gamma",
  theta1 = c(1/27, 1/270),
  theta2 = c(9, 90))

gamma2 <- RNGMIX(Dataset = "gamma2", n = n, Theta = Theta)

n <- c(80, 240, 80)

Theta <- rbind(pdf = "gamma",
  theta1 = c(1/20, 1, 1/20),
  theta2 = c(40, 6, 200))

gamma3 <- RNGMIX(Dataset = "gamma3 ", n = n, Theta = Theta)
@
The \texttt{gamma1\$Dataset}, \texttt{gamma2\$Dataset} and \texttt{gamma3\$Dataset} hold a list of data frames of size $n \times d$. See \texttt{help("RNGMIX")} in \pkg{rebmix} for details. The preprocessing is set to histogram, maximum number of components to $8$ and information criterion to AIC or BIC. The number of classes ranges from $30$ to $80$ and function \texttt{REBMIX} is called for the gamma parametric family type.
<<rebmix-code, split = FALSE, results = hide, keep.source = FALSE>>=
## Estimate number of components, component weights and component parameters.

gamma1est <- REBMIX(Dataset = gamma1$Dataset,
  Preprocessing = "histogram",
  cmax = 8,
  Criterion = c("AIC", "BIC"),
  Variables = "continuous",
  pdf = "gamma",
  K = 30:80)

gamma2est <- REBMIX(Dataset = gamma2$Dataset,
  Preprocessing = "histogram",
  cmax = 8,
  Criterion = "BIC",
  Variables = "continuous",
  pdf = "gamma",
  K = 30:80)

gamma3est <- REBMIX(Dataset = gamma3$Dataset,
  Preprocessing = "histogram",
  cmax = 8,
  Criterion = "BIC",
  Variables = "continuous",
  pdf = "gamma",
  K = 30:80)
@
See \texttt{help("REBMIX")} in \pkg{rebmix} for details about specifying arguments for the function returning an object of class \texttt{REBMIX}. List of data frames \texttt{w} contains component weights $w_{l}$ summing to $1$, \texttt{Theta} stands for a list of data frames containing parametric family types \texttt{pdfi}. One of \texttt{"normal"}, \texttt{"lognormal"}, \texttt{"Weibull"}, \texttt{"gamma"}, \texttt{"binomial"}, \texttt{"Poisson"} or \texttt{"Dirac"}. Component parameters \texttt{theta1.i} follow the parametric family types. One of $\mu_{il}$ for normal and lognormal distributions and $\theta_{il}$ for Weibull, gamma, binomial, Poisson and Dirac distributions. Component parameters \texttt{theta2.i} follow \texttt{theta1.i}. One of $\sigma_{il}$ for normal and lognormal distributions, $\beta_{il}$ for Weibull and gamma distributions and $p_{il}$ for binomial distribution. Character vector \texttt{Variables} contains types of variables. One of \texttt{"continuous"} or \texttt{"discrete"}.

In the \texttt{summary} data frame additional information about dataset, preprocessing, $c_{\mathrm{max}}$, information criterion type, $a_{\mathrm{r}}$, restraints type, optimal $c$, optimal $k$, $\bar{y}_{i0}$, optimal $h_{i}$, information criterion $\mathrm{IC}$ and log likelihood $\operatorname{log} L$ is stored. Position \texttt{pos} in the \texttt{summary} data frame at which log likelihood $\mathrm{log}\, L$ attains its maximum is available, too. See \texttt{help("summary.REBMIX")} for details.
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
summary(gamma1est)
@
The \texttt{plot} method delivers fitted finite mixture with the legend in Figure~\ref{figure:gamma2}.
\begin{figure}[htbp]\centering
<<gamma2-fig, fig = TRUE, pdf = TRUE, png = FALSE, eps = FALSE, height = 2.5, width = 5.5, echo = TRUE, results = hide, keep.source = FALSE>>=
plot(gamma2est, pos = 1, what = c("den", "dis"), ncol = 2, npts = 1000)
@
\caption{Gamma 2 dataset. Empirical density (circles) and predictive gamma mixture density in black solid line.}\label{figure:gamma2}
\end{figure}
The corresponding predictive gamma mixture parameters are given by the \texttt{coef} method.
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
coef(gamma2est)
@
For the details about specifying arguments for the \texttt{plot} and \texttt{coef} methods see \texttt{help("plot.REBMIX")} and \texttt{help("coef.REBMIX")}, respectively.

By calling the \texttt{boot.REBMIX} method \texttt{B} bootstrap datasets of length \texttt{n} are generated for the \texttt{x} object of class \texttt{REBMIX} at position \texttt{pos}, where bootstrap \texttt{Bootstrap} can be one of default \texttt{"parametric"} or \texttt{"nonparametric"}. Arguments \texttt{replace} and \texttt{prob} affect the nonparametric bootstrap only, see \texttt{help("sample")} and \cite{McLachlan_and_Peel_1997} for details about replacement and weighted bootstrap.
<<rebmix-code, split = FALSE, results = hide, keep.source = FALSE>>=
## Bootstrap finite mixture.
gamma3boot <- boot.REBMIX(x = gamma3est, pos = 1, Bootstrap = "p", B = 10, n = NULL, replace = TRUE, prob = NULL)
@
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
gamma3boot
@
The \texttt{gamma3boot} object of class \texttt{boot.REBMIX} holds a data frame \texttt{c} containing numbers $c$ of components for $B$ bootstrap datasets, standard error \texttt{c.se}, coefficient of variation \texttt{c.cv}, mode \texttt{c.mode} and mode probability \texttt{c.prob} of the numbers of components. Component weights \texttt{w}, component parameters \texttt{theta1.i} and \texttt{theta2.i}, standard errors \texttt{w.se}, \texttt{theta1.i.se} and \texttt{theta2.i.se} and coefficients of variation \texttt{w.cv}, \texttt{theta1.i.cv} and \texttt{theta2.i.cv} for those bootstrap datasets for which $c$ equals mode $c_{\mathrm{m}}$ are returned, too. See \texttt{help("boot.REBMIX")} in \pkg{rebmix} for details.
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
summary(gamma3boot)
@
\subsection{Poisson dataset}\label{subsec:poisson_dataset}
Dataset consists of $n = 600$ two~dimensional observations obtained by generating data points separately from each of three Poisson distributions. The component~dataset sizes and parameters, which are those studied in \citet{Jinwen_2009}, are displayed below
\begin{center}
\(\begin{array}{ll}
\bm{\theta}_{1} = (3, 2)^{\top} & n_{1} = 200 \\
\bm{\theta}_{2} = (9, 10)^{\top} & n_{2} = 200 \\
\bm{\theta}_{3} = (15, 16)^{\top} & n_{3} = 200
\end{array}\)
\end{center}
For the dataset \citet{Jinwen_2009} conduct $100$ experiments by selecting different initial values of the mixing proportions. In all the cases, the adaptive gradient BYY learning algorithm leads to the correct model selection, i.e., finally allocating the correct number of Poissons for the dataset. In the meantime, it also results in an estimate for each parameter in the original or true Poisson mixture which
generated the dataset. As the dataset of \citet{Jinwen_2009} can not exactly be reproduced, $100$ datasets are generated with random seeds $r_{\mathrm{seed}}$ ranging from $-1$ to $-100$.
<<rebmix-code, split = FALSE, results = hide, keep.source = FALSE>>=
#########################
##   Poisson dataset   ##
#########################

## Generate the Poisson dataset.

n <- c(200, 200, 200)

Theta <- rbind(rep("Poisson", 3), c(3, 9, 15), rep("Poisson", 3), c(2, 10, 16))

poisson <- RNGMIX(Dataset = paste("Poisson_", 1:100, sep = ""), n = n, Theta = Theta)
@
In total, $100$ finite mixture estimations are performed by calling the \texttt{REBMIX} function.
<<rebmix-code, split = FALSE, results = hide, keep.source = FALSE>>=
## Estimate number of components, component weights and component parameters.

poissonest <- REBMIX(Dataset = poisson$Dataset,
  Preprocessing = "histogram",
  cmax = 6,
  Criterion = "MDL5",
  Variables = rep("discrete", 2),
  pdf = rep("Poisson", 2),
  K = 1)

c <- as.numeric(poissonest$summary$c)
IC <- as.numeric(poissonest$summary$IC)
@
The results are as follows:
<<rebmix-code, split = FALSE, keep.source = FALSE>>=
## Visualize results.

summary(c)
summary(IC, digits = 5)
@
The \texttt{REBMIX} function predicts $\Sexpr{format(mean(c), digits = 3)}$ components on average, where probability of identifying exactly $c = 3$ components equals $\Sexpr{format(length(c[c == 3]) / length(c), digits = 3)}$. To plot the mixture in Figure~\ref{figure:poisson} the \texttt{plot} method is called.
\begin{figure}[htbp]\centering
<<poisson-fig, fig = TRUE, pdf = TRUE, png = FALSE, eps = FALSE, height = 5.0, width = 5.5, echo = TRUE, results = hide, keep.source = FALSE>>=
plot(poissonest, pos = 58, what = c("dens", "marg", "IC", "D", "logL"), nrow = 2, ncol = 3, npts = 1000)
@
\caption{Poisson dataset. Empirical densities (coloured large circles), predictive multivariate Poisson-Poisson mixture density (coloured small circles), empirical densities (circles), predictive univariate marginal Poisson mixture densities and progress charts (solid line).}\label{figure:poisson}
\end{figure}
<<rebmix-code, split = FALSE, echo = FALSE, results = hide, keep.source = FALSE>>=
rm(list = ls())
@
\section{Summary}\label{sec:summary}
The article presents the REBMIX algorithm and the \pkg{rebmix} package. Four datasets are studied on the x64 architecture. By applying the \pkg{tikzDevice} package \citep{tikzDevice_2013}, \LaTeX\ plots with legends can be obtained. The REBMIX algorithm can be used to assess the initial set of the unknown parameters and number of components for, e.g., the EM algorithm or as a standalone procedure that is a good compromise between the nonparametric and parametric methods to the finite mixture estimation. Its major advantages are robustness and time efficiency especially with the histogram preprocessing for all datasets sizes. The Parzen window and $k$-nearest neighbour preprocessing are more suitable for smaller datasets. Its advantages are more stressed if mixtures are composed of larger number of components. The \pkg{rebmix} package can be broadened to other parametric family types. The \texttt{RCLSMIX} method that enables class membership prediction is  available in the \pkg{rebmix} package, too. See \texttt{help("RCLSMIX")} for details. The REBMIX can thus also be used for pattern recognition.
\bibliography{rebmix}
\vspace{\baselineskip}\noindent\emph{Marko Nagode\\
University of Ljubljana\\
Faculty of Mechanical Engineering\\
A\v{s}ker\v{c}eva 6\\
1000 Ljubljana\\
Slovenia}\\
\href{mailto:Marko.Nagode@fs.uni-lj.si}{Marko.Nagode@fs.uni-lj.si}.
\end{document}
